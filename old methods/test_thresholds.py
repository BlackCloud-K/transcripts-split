import torch
import numpy as np
import json
import os
from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix
from train_hierarchical_bert import HierarchicalBertModel, HierarchicalBertConfig, HierarchicalDataset, collate_fn
from sentence_transformers import SentenceTransformer
from torch.utils.data import DataLoader

def test_thresholds():
    """æµ‹è¯•ä¸åŒé˜ˆå€¼å¯¹F1åˆ†æ•°çš„å½±å“"""
    
    # é…ç½®
    config = HierarchicalBertConfig()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # åŠ è½½æ¨¡å‹
    print("åŠ è½½æ¨¡å‹...")
    model = HierarchicalBertModel(config).to(device)
    
    # åŠ è½½æœ€ä½³æ¨¡å‹æƒé‡
    model_path = os.path.join(config.output_dir, 'best_model.pth')
    if os.path.exists(model_path):
        model.load_state_dict(torch.load(model_path, map_location=device))
        print(f"åŠ è½½æ¨¡å‹æƒé‡: {model_path}")
    else:
        print(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return
    
    # åŠ è½½å¥å­ç¼–ç å™¨
    sentence_encoder = SentenceTransformer(config.sentence_encoder)
    
    # å‡†å¤‡æ•°æ®é›†
    print("å‡†å¤‡æ•°æ®é›†...")
    dataset = HierarchicalDataset(
        config.data_path, 
        sentence_encoder, 
        config.max_sentences
    )
    
    # å‡†å¤‡æ•°æ®
    processed_data = dataset.prepare_hierarchical_data()
    
    # åˆ†å‰²æ•°æ®ï¼ˆä½¿ç”¨ç›¸åŒçš„éšæœºç§å­ï¼‰
    from sklearn.model_selection import train_test_split
    has_chunk = [sum(d['labels']) > 0 for d in processed_data]
    train_data, val_data = train_test_split(
        processed_data, 
        test_size=1-config.train_split, 
        stratify=has_chunk,
        random_state=config.seed
    )
    
    # åˆ›å»ºéªŒè¯æ•°æ®åŠ è½½å™¨
    val_dataloader = DataLoader(
        val_data, 
        batch_size=config.batch_size, 
        shuffle=False, 
        collate_fn=collate_fn
    )
    
    # æµ‹è¯•ä¸åŒé˜ˆå€¼
    thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
    results = []
    
    print(f"\nğŸ” æµ‹è¯•ä¸åŒé˜ˆå€¼...")
    print(f"éªŒè¯é›†å¤§å°: {len(val_data)} æ–‡ä»¶")
    
    model.eval()
    with torch.no_grad():
        # æ”¶é›†æ‰€æœ‰é¢„æµ‹å’Œæ ‡ç­¾
        all_preds = []
        all_labels = []
        all_lengths = []
        
        for batch in val_dataloader:
            embeddings = batch['embeddings'].to(device)
            labels = batch['labels'].to(device)
            lengths = batch['lengths']
            
            # å‰å‘ä¼ æ’­
            preds = model(embeddings, lengths)
            
            # æ”¶é›†ç»“æœ
            for i, length in enumerate(lengths):
                all_preds.append(preds[i][:length])
                all_labels.append(labels[i][:length])
                all_lengths.append(length)
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        all_preds_np = []
        all_labels_np = []
        for i, length in enumerate(all_lengths):
            all_preds_np.extend(all_preds[i].cpu().numpy())
            all_labels_np.extend(all_labels[i].cpu().numpy())
        
        all_preds_np = np.array(all_preds_np)
        all_labels_np = np.array(all_labels_np)
        
        print(f"æ€»å¥å­æ•°: {len(all_preds_np)}")
        print(f"æ­£æ ·æœ¬æ•°: {np.sum(all_labels_np)}")
        print(f"è´Ÿæ ·æœ¬æ•°: {len(all_labels_np) - np.sum(all_labels_np)}")
        
        # æµ‹è¯•æ¯ä¸ªé˜ˆå€¼
        for threshold in thresholds:
            # åº”ç”¨é˜ˆå€¼
            binary_preds = (all_preds_np > threshold).astype(int)
            
            # è®¡ç®—æŒ‡æ ‡
            precision, recall, f1, _ = precision_recall_fscore_support(
                all_labels_np, binary_preds, average='binary', zero_division=0
            )
            acc = accuracy_score(all_labels_np, binary_preds)
            
            # æ··æ·†çŸ©é˜µ
            cm = confusion_matrix(all_labels_np, binary_preds)
            if cm.shape == (2, 2):
                tn, fp, fn, tp = cm.ravel()
            else:
                tp = np.sum((all_labels_np == 1) & (binary_preds == 1))
                tn = np.sum((all_labels_np == 0) & (binary_preds == 0))
                fp = np.sum((all_labels_np == 0) & (binary_preds == 1))
                fn = np.sum((all_labels_np == 1) & (binary_preds == 0))
            
            results.append({
                'threshold': float(threshold),
                'precision': float(precision),
                'recall': float(recall),
                'f1': float(f1),
                'accuracy': float(acc),
                'true_positives': int(tp),
                'false_positives': int(fp),
                'false_negatives': int(fn),
                'true_negatives': int(tn)
            })
            
            print(f"é˜ˆå€¼ {threshold:.1f}: F1={f1:.4f}, P={precision:.4f}, R={recall:.4f}, TP={tp}, FP={fp}, FN={fn}")
    
    # æ‰¾åˆ°æœ€ä½³é˜ˆå€¼
    best_result = max(results, key=lambda x: x['f1'])
    print(f"\nğŸ† æœ€ä½³ç»“æœ:")
    print(f"   é˜ˆå€¼: {best_result['threshold']:.1f}")
    print(f"   F1: {best_result['f1']:.4f}")
    print(f"   Precision: {best_result['precision']:.4f}")
    print(f"   Recall: {best_result['recall']:.4f}")
    print(f"   çœŸæ­£ä¾‹: {best_result['true_positives']}")
    print(f"   å‡æ­£ä¾‹: {best_result['false_positives']}")
    print(f"   å‡è´Ÿä¾‹: {best_result['false_negatives']}")
    
    # ä¿å­˜ç»“æœ
    with open('threshold_test_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nç»“æœå·²ä¿å­˜åˆ°: threshold_test_results.json")
    
    return results

def analyze_predictions():
    """åˆ†æé¢„æµ‹ç»“æœåˆ†å¸ƒ"""
    print(f"\nğŸ“Š åˆ†æé¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ...")
    
    # é…ç½®
    config = HierarchicalBertConfig()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # åŠ è½½æ¨¡å‹
    model = HierarchicalBertModel(config).to(device)
    model_path = os.path.join(config.output_dir, 'best_model.pth')
    if os.path.exists(model_path):
        model.load_state_dict(torch.load(model_path, map_location=device))
    
    # åŠ è½½å¥å­ç¼–ç å™¨
    sentence_encoder = SentenceTransformer(config.sentence_encoder)
    
    # å‡†å¤‡æ•°æ®é›†
    dataset = HierarchicalDataset(
        config.data_path, 
        sentence_encoder, 
        config.max_sentences
    )
    processed_data = dataset.prepare_hierarchical_data()
    
    # åˆ†å‰²æ•°æ®
    from sklearn.model_selection import train_test_split
    has_chunk = [sum(d['labels']) > 0 for d in processed_data]
    train_data, val_data = train_test_split(
        processed_data, 
        test_size=1-config.train_split, 
        stratify=has_chunk,
        random_state=config.seed
    )
    
    # åˆ›å»ºéªŒè¯æ•°æ®åŠ è½½å™¨
    val_dataloader = DataLoader(
        val_data, 
        batch_size=config.batch_size, 
        shuffle=False, 
        collate_fn=collate_fn
    )
    
    # æ”¶é›†é¢„æµ‹æ¦‚ç‡
    all_preds = []
    all_labels = []
    
    model.eval()
    with torch.no_grad():
        for batch in val_dataloader:
            embeddings = batch['embeddings'].to(device)
            labels = batch['labels'].to(device)
            lengths = batch['lengths']
            
            preds = model(embeddings, lengths)
            
            for i, length in enumerate(lengths):
                all_preds.extend(preds[i][:length].cpu().numpy())
                all_labels.extend(labels[i][:length].cpu().numpy())
    
    all_preds = np.array(all_preds)
    all_labels = np.array(all_labels)
    
    # åˆ†æåˆ†å¸ƒ
    positive_preds = all_preds[all_labels == 1]
    negative_preds = all_preds[all_labels == 0]
    
    print(f"æ­£æ ·æœ¬é¢„æµ‹åˆ†å¸ƒ:")
    print(f"   æ•°é‡: {len(positive_preds)}")
    print(f"   å‡å€¼: {np.mean(positive_preds):.4f}")
    print(f"   æ ‡å‡†å·®: {np.std(positive_preds):.4f}")
    print(f"   æœ€å°å€¼: {np.min(positive_preds):.4f}")
    print(f"   æœ€å¤§å€¼: {np.max(positive_preds):.4f}")
    print(f"   ä¸­ä½æ•°: {np.median(positive_preds):.4f}")
    
    print(f"\nè´Ÿæ ·æœ¬é¢„æµ‹åˆ†å¸ƒ:")
    print(f"   æ•°é‡: {len(negative_preds)}")
    print(f"   å‡å€¼: {np.mean(negative_preds):.4f}")
    print(f"   æ ‡å‡†å·®: {np.std(negative_preds):.4f}")
    print(f"   æœ€å°å€¼: {np.min(negative_preds):.4f}")
    print(f"   æœ€å¤§å€¼: {np.max(negative_preds):.4f}")
    print(f"   ä¸­ä½æ•°: {np.median(negative_preds):.4f}")
    
    # è®¡ç®—é‡å ç¨‹åº¦
    overlap_threshold = 0.5
    positive_above_threshold = np.sum(positive_preds > overlap_threshold)
    negative_above_threshold = np.sum(negative_preds > overlap_threshold)
    
    print(f"\né‡å åˆ†æ (é˜ˆå€¼={overlap_threshold}):")
    print(f"   æ­£æ ·æœ¬ > {overlap_threshold}: {positive_above_threshold}/{len(positive_preds)} ({positive_above_threshold/len(positive_preds)*100:.1f}%)")
    print(f"   è´Ÿæ ·æœ¬ > {overlap_threshold}: {negative_above_threshold}/{len(negative_preds)} ({negative_above_threshold/len(negative_preds)*100:.1f}%)")

if __name__ == "__main__":
    print("ğŸ” å¼€å§‹é˜ˆå€¼æµ‹è¯•...")
    results = test_thresholds()
    
    print("\n" + "="*50)
    analyze_predictions() 