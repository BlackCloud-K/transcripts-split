import torch
import numpy as np
import json
import os
from dataclasses import dataclass
from typing import List, Dict, Tuple
from tqdm import tqdm
import wandb
from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix
from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification,
    TrainingArguments, 
    Trainer, 
    TrainerCallback,
    DataCollatorWithPadding
)
from peft import LoraConfig, get_peft_model
import matplotlib.pyplot as plt
import seaborn as sns

@dataclass
class BalancedTrainingConfig:
    """å¹³è¡¡è®­ç»ƒé…ç½®"""
    # æ¨¡å‹é…ç½®
    model_name: str = "allenai/longformer-base-4096"
    max_length: int = 1024
    
    # è®­ç»ƒé…ç½®
    batch_size: int = 4
    learning_rate: float = 2e-5
    num_epochs: int = 3  # ä¿æŒ3ä¸ªepoch
    warmup_steps: int = 100
    weight_decay: float = 0.01
    
    # LoRAé…ç½®
    lora_r: int = 8
    lora_alpha: int = 32
    lora_dropout: float = 0.1
    
    # æ•°æ®é…ç½®
    train_split: float = 0.8
    seed: int = 42
    context_window: int = 15
    
    # ä¸å¹³è¡¡å¤„ç†é…ç½®
    positive_weight: float = 2.0  # æ­£æ ·æœ¬æƒé‡ï¼ˆæ•°æ®å·²å¹³è¡¡ï¼Œä½¿ç”¨è¾ƒå°æƒé‡ï¼‰
    use_stratified_sampling: bool = True
    use_weighted_loss: bool = False  # å…ˆå°è¯•ä¸ä½¿ç”¨åŠ æƒæŸå¤±
    
    # è·¯å¾„é…ç½®
    data_path: str = "training_data.json"
    output_dir: str = "./chunk_model_balanced_output"
    model_save_dir: str = "./chunk_model_balanced_final"
    
    # å…¶ä»–é…ç½®
    use_wandb: bool = True
    gradient_accumulation_steps: int = 4
    fp16: bool = True

class DetailedProgressCallback(TrainerCallback):
    """è¯¦ç»†çš„è¿›åº¦å›è°ƒå‡½æ•°"""
    def __init__(self, total_steps: int):
        self.total_steps = total_steps
        self.current_step = 0
        self.epoch = 0
        
    def on_step_begin(self, args, state, control, **kwargs):
        self.current_step = state.global_step
        
    def on_epoch_begin(self, args, state, control, **kwargs):
        self.epoch = state.epoch
        
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs:
            progress = (self.current_step / self.total_steps) * 100
            print(f"ğŸ“Š è®­ç»ƒè¿›åº¦: {progress:.1f}% | æ­¥éª¤: {self.current_step}/{self.total_steps} | Epoch: {self.epoch:.2f}")
            
            # è¯¦ç»†æŒ‡æ ‡æ˜¾ç¤º
            if 'loss' in logs:
                print(f"   ğŸ“ˆ è®­ç»ƒæŸå¤±: {logs['loss']:.4f}")
            if 'eval_loss' in logs:
                print(f"   ğŸ“‰ éªŒè¯æŸå¤±: {logs['eval_loss']:.4f}")
            if 'eval_f1' in logs:
                print(f"   ğŸ¯ F1åˆ†æ•°: {logs['eval_f1']:.4f}")
            if 'eval_precision' in logs:
                print(f"   ğŸ¯ Precision: {logs['eval_precision']:.4f}")
            if 'eval_recall' in logs:
                print(f"   ğŸ¯ Recall: {logs['eval_recall']:.4f}")
            if 'eval_accuracy' in logs:
                print(f"   âœ… å‡†ç¡®ç‡: {logs['eval_accuracy']:.4f}")
            
            # æ˜¾ç¤ºæ­£æ ·æœ¬é¢„æµ‹æƒ…å†µ
            if 'eval_true_positives' in logs:
                print(f"   ğŸŸ¢ çœŸæ­£ä¾‹: {logs['eval_true_positives']}")
            if 'eval_false_positives' in logs:
                print(f"   ğŸ”´ å‡æ­£ä¾‹: {logs['eval_false_positives']}")
            if 'eval_false_negatives' in logs:
                print(f"   ğŸŸ¡ å‡è´Ÿä¾‹: {logs['eval_false_negatives']}")
                
            print("-" * 60)

class BalancedSentenceLevelDataset:
    """å¹³è¡¡çš„å¥å­çº§åˆ«æ•°æ®é›†"""
    
    def __init__(self, data_path: str, tokenizer, max_length: int = 1024, context_window: int = 15):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.context_window = context_window
        self.data = self.load_data(data_path)
        
    def load_data(self, data_path: str) -> List[Dict]:
        """åŠ è½½æ•°æ®"""
        with open(data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"åŠ è½½äº† {len(data)} ä¸ªtranscriptæ–‡ä»¶")
        return data
    
    def prepare_sentence_level_data(self) -> List[Dict]:
        """å‡†å¤‡å¥å­çº§åˆ«çš„æ•°æ®"""
        processed_data = []
        
        for sample in tqdm(self.data, desc="å‡†å¤‡å¥å­çº§æ•°æ®"):
            sentences = sample['sentences']
            labels = sample['labels']
            
            # ä¸ºæ¯ä¸ªå¥å­åˆ›å»ºä¸€ä¸ªæ ·æœ¬
            for i, (sentence, label) in enumerate(zip(sentences, labels)):
                # åˆ›å»ºä¸Šä¸‹æ–‡çª—å£
                context_start = max(0, i - self.context_window)
                context_end = min(len(sentences), i + self.context_window + 1)
                context_sentences = sentences[context_start:context_end]
                
                # åˆå¹¶ä¸Šä¸‹æ–‡
                context_text = " ".join(context_sentences)
                
                # åˆ›å»ºè®­ç»ƒæ ·æœ¬
                processed_data.append({
                    'text': context_text,
                    'label': label,
                    'sentence_index': i,
                    'total_sentences': len(sentences),
                    'context_size': len(context_sentences),
                    'is_chunk_start': label == 1
                })
        
        # æ•°æ®å¹³è¡¡å¤„ç†
        positive_samples = [d for d in processed_data if d['label'] == 1]
        negative_samples = [d for d in processed_data if d['label'] == 0]
        
        print(f"ğŸ“Š åŸå§‹æ•°æ®åˆ†å¸ƒ:")
        print(f"   æ­£æ ·æœ¬æ•°: {len(positive_samples)}")
        print(f"   è´Ÿæ ·æœ¬æ•°: {len(negative_samples)}")
        print(f"   æ­£è´Ÿæ¯”ä¾‹: 1:{len(negative_samples)/len(positive_samples):.1f}")
        
        # ç›®æ ‡ï¼šæ¯3ä¸ªæ ·æœ¬ä¸­è‡³å°‘æœ‰1ä¸ªæ­£æ ·æœ¬ (æ­£æ ·æœ¬å æ¯”33.3%)
        target_positive_ratio = 0.333
        target_negative_ratio = 1 - target_positive_ratio
        
        # è®¡ç®—éœ€è¦çš„è´Ÿæ ·æœ¬æ•°é‡
        target_negative_count = int(len(positive_samples) * target_negative_ratio / target_positive_ratio)
        
        # éšæœºé‡‡æ ·è´Ÿæ ·æœ¬
        if len(negative_samples) > target_negative_count:
            negative_samples = np.random.choice(negative_samples, target_negative_count, replace=False)
            if isinstance(negative_samples, np.ndarray):
                negative_samples = negative_samples.tolist()
        
        # åˆå¹¶å¹³è¡¡åçš„æ•°æ®
        balanced_data = positive_samples + negative_samples
        
        # æ‰“ä¹±æ•°æ®é¡ºåº
        np.random.shuffle(balanced_data)
        
        # è¯¦ç»†çš„æ•°æ®åˆ†å¸ƒåˆ†æ
        total_samples = len(balanced_data)
        final_positive_samples = sum(d['label'] for d in balanced_data)
        final_negative_samples = total_samples - final_positive_samples
        
        print(f"ğŸ“Š å¹³è¡¡åæ•°æ®åˆ†å¸ƒ:")
        print(f"   æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"   æ­£æ ·æœ¬æ•°: {final_positive_samples} ({final_positive_samples/total_samples*100:.1f}%)")
        print(f"   è´Ÿæ ·æœ¬æ•°: {final_negative_samples} ({final_negative_samples/total_samples*100:.1f}%)")
        print(f"   æ­£è´Ÿæ¯”ä¾‹: 1:{final_negative_samples/final_positive_samples:.1f}")
        print(f"   å¹³å‡ä¸Šä¸‹æ–‡å¤§å°: {np.mean([d['context_size'] for d in balanced_data]):.1f} å¥å­")
        
        return balanced_data
    
    def tokenize_function(self, examples):
        """tokenizeå‡½æ•°"""
        tokenized = self.tokenizer(
            examples['text'],
            truncation=True,
            padding=True,
            max_length=self.max_length,
            return_tensors=None
        )
        
        # ç¡®ä¿labelså­—æ®µè¢«ä¿ç•™
        tokenized['labels'] = examples['label']
        
        return tokenized

def compute_detailed_metrics(pred):
    """è®¡ç®—è¯¦ç»†çš„è¯„ä¼°æŒ‡æ ‡"""
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    
    # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
    labels = labels.astype(int)
    preds = preds.astype(int)
    
    # åŸºç¡€æŒ‡æ ‡
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary', zero_division=0)
    acc = accuracy_score(labels, preds)
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(labels, preds)
    
    if cm.shape == (2, 2):
        tn, fp, fn, tp = cm.ravel()
    else:
        # å¦‚æœæ··æ·†çŸ©é˜µä¸æ˜¯2x2ï¼Œæ‰‹åŠ¨è®¡ç®—
        tp = np.sum((labels == 1) & (preds == 1))
        tn = np.sum((labels == 0) & (preds == 0))
        fp = np.sum((labels == 0) & (preds == 1))
        fn = np.sum((labels == 1) & (preds == 0))
    
    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
    precision_neg, recall_neg, f1_neg, _ = precision_recall_fscore_support(labels, preds, average='binary', pos_label=0, zero_division=0)
    precision_pos, recall_pos, f1_pos, _ = precision_recall_fscore_support(labels, preds, average='binary', pos_label=1, zero_division=0)
    
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall,
        'precision_negative': precision_neg,
        'recall_negative': recall_neg,
        'f1_negative': f1_neg,
        'precision_positive': precision_pos,
        'recall_positive': recall_pos,
        'f1_positive': f1_pos,
        'true_negatives': int(tn),
        'false_positives': int(fp),
        'false_negatives': int(fn),
        'true_positives': int(tp),
        'eval_true_positives': int(tp),
        'eval_false_positives': int(fp),
        'eval_false_negatives': int(fn),
    }

def setup_balanced_model_and_tokenizer(config: BalancedTrainingConfig):
    """è®¾ç½®å¹³è¡¡çš„æ¨¡å‹å’Œtokenizer"""
    print("åŠ è½½tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    
    print("åŠ è½½æ¨¡å‹...")
    model = AutoModelForSequenceClassification.from_pretrained(
        config.model_name,
        num_labels=2,
        ignore_mismatched_sizes=True
    )
    
    # é…ç½®LoRA
    print("é…ç½®LoRA...")
    lora_config = LoraConfig(
        r=config.lora_r,
        lora_alpha=config.lora_alpha,
        target_modules=["query", "value"],
        lora_dropout=config.lora_dropout,
        bias="none",
        task_type="SEQ_CLS"
    )
    
    model = get_peft_model(model, lora_config)
    
    # æ‰“å°å¯è®­ç»ƒå‚æ•°
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    all_params = sum(p.numel() for p in model.parameters())
    print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({trainable_params/all_params*100:.2f}%)")
    
    return model, tokenizer

def create_simple_weighted_loss_function(positive_weight: float = 2.0):
    """åˆ›å»ºç®€å•åŠ æƒæŸå¤±å‡½æ•°"""
    def weighted_loss(model, inputs, num_items_in_batch=None, return_outputs=False):
        # è·å–logitså’Œlabels
        outputs = model(**inputs)
        logits = outputs.logits
        labels = inputs.get("labels")
        
        # é‡å¡‘ä¸º(batch_size * seq_len, num_classes)
        logits_flat = logits.view(-1, 2)
        labels_flat = labels.view(-1)
        
        # è®¡ç®—åŠ æƒäº¤å‰ç†µæŸå¤±
        loss_fct = torch.nn.CrossEntropyLoss(
            weight=torch.tensor([1.0, positive_weight], device=logits.device)
        )
        loss = loss_fct(logits_flat, labels_flat)
        
        if return_outputs:
            return loss, outputs
        else:
            return loss
    
    return weighted_loss

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    
    # é…ç½®
    config = BalancedTrainingConfig()
    
    # æ£€æŸ¥GPU
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆå§‹åŒ–wandb
    if config.use_wandb:
        try:
            wandb.init(project="comedy-chunk-detection", name="longformer-balanced-training")
        except:
            print("Wandbåˆå§‹åŒ–å¤±è´¥ï¼Œç»§ç»­è®­ç»ƒ...")
    
    # åŠ è½½æ¨¡å‹å’Œtokenizer
    model, tokenizer = setup_balanced_model_and_tokenizer(config)
    
    # å‡†å¤‡æ•°æ®é›†
    print("å‡†å¤‡æ•°æ®é›†...")
    dataset = BalancedSentenceLevelDataset(
        config.data_path, 
        tokenizer, 
        config.max_length, 
        config.context_window
    )
    
    # å‡†å¤‡å¥å­çº§æ•°æ®
    processed_data = dataset.prepare_sentence_level_data()
    
    # åˆ†å±‚åˆ†å‰²æ•°æ®
    if config.use_stratified_sampling:
        print("ä½¿ç”¨åˆ†å±‚é‡‡æ ·åˆ†å‰²æ•°æ®...")
        labels = [d['label'] for d in processed_data]
        
        # æ£€æŸ¥æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°é‡
        unique_labels, counts = np.unique(labels, return_counts=True)
        print(f"ç±»åˆ«åˆ†å¸ƒ: {dict(zip(unique_labels, counts))}")
        
        # å¦‚æœæŸä¸ªç±»åˆ«æ ·æœ¬å¤ªå°‘ï¼Œä½¿ç”¨éšæœºåˆ†å‰²
        if min(counts) < 2:
            print(f"è­¦å‘Šï¼šæŸä¸ªç±»åˆ«æ ·æœ¬æ•°é‡ä¸è¶³ï¼ˆæœ€å°‘{min(counts)}ä¸ªï¼‰ï¼Œåˆ‡æ¢åˆ°éšæœºåˆ†å‰²")
            train_data, val_data = train_test_split(
                processed_data, 
                test_size=1-config.train_split, 
                random_state=config.seed
            )
        else:
            train_data, val_data = train_test_split(
                processed_data, 
                test_size=1-config.train_split, 
                stratify=labels,
                random_state=config.seed
            )
    else:
        print("ä½¿ç”¨éšæœºåˆ†å‰²æ•°æ®...")
        train_data, val_data = train_test_split(
            processed_data, 
            test_size=1-config.train_split, 
            random_state=config.seed
        )
    
    print(f"è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬")
    print(f"éªŒè¯é›†: {len(val_data)} æ ·æœ¬")
    
    # æ˜¾ç¤ºåˆ†å‰²åçš„æ•°æ®åˆ†å¸ƒ
    train_pos = sum(d['label'] for d in train_data)
    train_neg = len(train_data) - train_pos
    val_pos = sum(d['label'] for d in val_data)
    val_neg = len(val_data) - val_pos
    
    print(f"è®­ç»ƒé›†åˆ†å¸ƒ: æ­£æ ·æœ¬ {train_pos} ({train_pos/len(train_data)*100:.1f}%), è´Ÿæ ·æœ¬ {train_neg} ({train_neg/len(train_data)*100:.1f}%)")
    print(f"éªŒè¯é›†åˆ†å¸ƒ: æ­£æ ·æœ¬ {val_pos} ({val_pos/len(val_data)*100:.1f}%), è´Ÿæ ·æœ¬ {val_neg} ({val_neg/len(val_data)*100:.1f}%)")
    
    # åˆ›å»ºHuggingFaceæ•°æ®é›†
    train_dataset = Dataset.from_list(train_data)
    val_dataset = Dataset.from_list(val_data)
    
    # Tokenizeæ•°æ®é›†
    train_dataset = train_dataset.map(
        dataset.tokenize_function, 
        batched=True, 
        remove_columns=train_dataset.column_names
    )
    val_dataset = val_dataset.map(
        dataset.tokenize_function, 
        batched=True, 
        remove_columns=val_dataset.column_names
    )
    
    # æ•°æ®æ”¶é›†å™¨
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
    
    # è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=config.output_dir,
        learning_rate=config.learning_rate,
        per_device_train_batch_size=config.batch_size,
        per_device_eval_batch_size=config.batch_size,
        num_train_epochs=config.num_epochs,
        weight_decay=config.weight_decay,
        eval_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        metric_for_best_model="f1",  # ä½¿ç”¨F1ä½œä¸ºä¸»è¦æŒ‡æ ‡
        greater_is_better=True,
        warmup_steps=config.warmup_steps,
        logging_steps=10,
        save_total_limit=2,
        report_to="wandb" if wandb.run is not None else None,
        remove_unused_columns=False,
        fp16=config.fp16,
        gradient_accumulation_steps=config.gradient_accumulation_steps,
        logging_dir="./logs",
        log_level="info",
        dataloader_pin_memory=False,
        dataloader_num_workers=0,
    )
    
    # è®¡ç®—æ€»è®­ç»ƒæ­¥æ•°
    total_steps = len(train_dataset) // (config.batch_size * config.gradient_accumulation_steps) * config.num_epochs
    print(f"é¢„è®¡æ€»è®­ç»ƒæ­¥æ•°: {total_steps}")
    
    # åˆ›å»ºè¿›åº¦å›è°ƒ
    progress_callback = DetailedProgressCallback(total_steps)
    
    # åˆ›å»ºTrainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_detailed_metrics,
        callbacks=[progress_callback],
    )
    
    # å¦‚æœä½¿ç”¨åŠ æƒæŸå¤±ï¼Œæ›¿æ¢æŸå¤±å‡½æ•°
    if config.use_weighted_loss:
        print(f"ä½¿ç”¨åŠ æƒæŸå¤±å‡½æ•°")
        print(f"  æ­£æ ·æœ¬æƒé‡: {config.positive_weight}")
        trainer.compute_loss = create_simple_weighted_loss_function(config.positive_weight)
    
    # å¼€å§‹è®­ç»ƒ
    print("å¼€å§‹å¹³è¡¡è®­ç»ƒ...")
    trainer.train()
    
    # ä¿å­˜æ¨¡å‹
    print("ä¿å­˜æ¨¡å‹...")
    trainer.save_model(config.model_save_dir)
    tokenizer.save_pretrained(config.model_save_dir)
    
    # æœ€ç»ˆè¯„ä¼°
    print("æœ€ç»ˆè¯„ä¼°...")
    results = trainer.evaluate()
    print(f"æœ€ç»ˆç»“æœ: {results}")
    
    # ä¿å­˜è®­ç»ƒç»“æœ
    with open("balanced_training_results.json", "w") as f:
        json.dump(results, f, indent=2)
    
    # åˆ›å»ºæ··æ·†çŸ©é˜µå¯è§†åŒ–
    print("åˆ›å»ºæ··æ·†çŸ©é˜µ...")
    predictions = trainer.predict(val_dataset)
    preds = predictions.predictions.argmax(-1)
    labels = predictions.label_ids
    
    cm = confusion_matrix(labels, preds)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['éChunkå¼€å§‹', 'Chunkå¼€å§‹'],
                yticklabels=['éChunkå¼€å§‹', 'Chunkå¼€å§‹'])
    plt.title('æ··æ·†çŸ©é˜µ')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print("è®­ç»ƒå®Œæˆï¼")
    print(f"æ¨¡å‹ä¿å­˜åœ¨: {config.model_save_dir}")
    print(f"ç»“æœä¿å­˜åœ¨: balanced_training_results.json")
    print(f"æ··æ·†çŸ©é˜µä¿å­˜åœ¨: confusion_matrix.png")

if __name__ == "__main__":
    main() 