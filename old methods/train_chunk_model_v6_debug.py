import torch
import numpy as np
import json
import os
from dataclasses import dataclass
from typing import List, Dict, Tuple
from tqdm import tqdm
import wandb
from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix
from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification,
    TrainingArguments, 
    Trainer, 
    TrainerCallback,
    DataCollatorWithPadding
)
from peft import LoraConfig, get_peft_model
import matplotlib.pyplot as plt
import seaborn as sns

@dataclass
class DebugTrainingConfig:
    """è°ƒè¯•è®­ç»ƒé…ç½®"""
    # æ¨¡å‹é…ç½®
    model_name: str = "allenai/longformer-base-4096"
    max_length: int = 512  # å‡å°‘é•¿åº¦ï¼Œé¿å…å†…å­˜é—®é¢˜
    
    # è®­ç»ƒé…ç½®
    batch_size: int = 2  # å‡å°‘batch size
    learning_rate: float = 1e-4  # å¢åŠ å­¦ä¹ ç‡
    num_epochs: int = 5  # å¢åŠ epochæ•°
    warmup_steps: int = 50
    weight_decay: float = 0.01
    
    # LoRAé…ç½®
    lora_r: int = 16  # å¢åŠ rank
    lora_alpha: int = 32
    lora_dropout: float = 0.1
    
    # æ•°æ®é…ç½®
    train_split: float = 0.8
    seed: int = 42
    context_window: int = 5  # å‡å°‘ä¸Šä¸‹æ–‡çª—å£
    
    # ä¸å¹³è¡¡å¤„ç†é…ç½®
    use_stratified_sampling: bool = True
    use_weighted_loss: bool = False
    
    # è·¯å¾„é…ç½®
    data_path: str = "training_data.json"
    output_dir: str = "./chunk_model_debug_output"
    model_save_dir: str = "./chunk_model_debug_final"
    
    # å…¶ä»–é…ç½®
    use_wandb: bool = True
    gradient_accumulation_steps: int = 2
    fp16: bool = False  # å…³é—­fp16é¿å…ç²¾åº¦é—®é¢˜

class DebugCallback(TrainerCallback):
    """è°ƒè¯•å›è°ƒå‡½æ•°"""
    def __init__(self):
        self.step = 0
        
    def on_step_end(self, args, state, control, logs=None, **kwargs):
        self.step += 1
        if self.step % 10 == 0 and logs:
            print(f"æ­¥éª¤ {self.step}: æŸå¤± = {logs.get('loss', 'N/A'):.4f}")
    
    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        if metrics:
            print(f"ğŸ” è¯„ä¼°ç»“æœ:")
            print(f"  æŸå¤±: {metrics.get('eval_loss', 'N/A'):.4f}")
            print(f"  F1: {metrics.get('eval_f1', 'N/A'):.4f}")
            print(f"  å‡†ç¡®ç‡: {metrics.get('eval_accuracy', 'N/A'):.4f}")
            print(f"  çœŸæ­£ä¾‹: {metrics.get('eval_true_positives', 'N/A')}")
            print(f"  å‡æ­£ä¾‹: {metrics.get('eval_false_positives', 'N/A')}")
            print(f"  å‡è´Ÿä¾‹: {metrics.get('eval_false_negatives', 'N/A')}")

class DebugDataset:
    """è°ƒè¯•æ•°æ®é›†"""
    
    def __init__(self, data_path: str, tokenizer, max_length: int = 512, context_window: int = 5):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.context_window = context_window
        self.data = self.load_data(data_path)
        
    def load_data(self, data_path: str) -> List[Dict]:
        with open(data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"åŠ è½½äº† {len(data)} ä¸ªtranscriptæ–‡ä»¶")
        return data
    
    def prepare_sentence_level_data(self) -> List[Dict]:
        """å‡†å¤‡å¥å­çº§åˆ«çš„æ•°æ® - ç®€åŒ–ç‰ˆæœ¬"""
        processed_data = []
        
        for sample in tqdm(self.data, desc="å‡†å¤‡å¥å­çº§æ•°æ®"):
            sentences = sample['sentences']
            labels = sample['labels']
            
            # ä¸ºæ¯ä¸ªå¥å­åˆ›å»ºä¸€ä¸ªæ ·æœ¬
            for i, (sentence, label) in enumerate(zip(sentences, labels)):
                # åˆ›å»ºç®€åŒ–çš„ä¸Šä¸‹æ–‡çª—å£
                context_start = max(0, i - self.context_window)
                context_end = min(len(sentences), i + self.context_window + 1)
                context_sentences = sentences[context_start:context_end]
                
                # åˆå¹¶ä¸Šä¸‹æ–‡
                context_text = " ".join(context_sentences)
                
                # åˆ›å»ºè®­ç»ƒæ ·æœ¬
                processed_data.append({
                    'text': context_text,
                    'label': label,
                    'sentence_index': i,
                    'total_sentences': len(sentences),
                    'context_size': len(context_sentences),
                    'is_chunk_start': label == 1
                })
        
        # æ•°æ®å¹³è¡¡å¤„ç†
        positive_samples = [d for d in processed_data if d['label'] == 1]
        negative_samples = [d for d in processed_data if d['label'] == 0]
        
        print(f"ğŸ“Š åŸå§‹æ•°æ®åˆ†å¸ƒ:")
        print(f"   æ­£æ ·æœ¬æ•°: {len(positive_samples)}")
        print(f"   è´Ÿæ ·æœ¬æ•°: {len(negative_samples)}")
        print(f"   æ­£è´Ÿæ¯”ä¾‹: 1:{len(negative_samples)/len(positive_samples):.1f}")
        
        # ç›®æ ‡ï¼šæ¯3ä¸ªæ ·æœ¬ä¸­è‡³å°‘æœ‰1ä¸ªæ­£æ ·æœ¬
        target_positive_ratio = 0.333
        target_negative_ratio = 1 - target_positive_ratio
        target_negative_count = int(len(positive_samples) * target_negative_ratio / target_positive_ratio)
        
        # éšæœºé‡‡æ ·è´Ÿæ ·æœ¬
        if len(negative_samples) > target_negative_count:
            negative_samples = np.random.choice(negative_samples, target_negative_count, replace=False)
            if isinstance(negative_samples, np.ndarray):
                negative_samples = negative_samples.tolist()
        
        # åˆå¹¶å¹³è¡¡åçš„æ•°æ®
        balanced_data = positive_samples + negative_samples
        np.random.shuffle(balanced_data)
        
        # è¯¦ç»†çš„æ•°æ®åˆ†å¸ƒåˆ†æ
        total_samples = len(balanced_data)
        final_positive_samples = sum(d['label'] for d in balanced_data)
        final_negative_samples = total_samples - final_positive_samples
        
        print(f"ğŸ“Š å¹³è¡¡åæ•°æ®åˆ†å¸ƒ:")
        print(f"   æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"   æ­£æ ·æœ¬æ•°: {final_positive_samples} ({final_positive_samples/total_samples*100:.1f}%)")
        print(f"   è´Ÿæ ·æœ¬æ•°: {final_negative_samples} ({final_negative_samples/total_samples*100:.1f}%)")
        print(f"   æ­£è´Ÿæ¯”ä¾‹: 1:{final_negative_samples/final_positive_samples:.1f}")
        
        # éªŒè¯æ ‡ç­¾åˆ†å¸ƒ
        labels = [d['label'] for d in balanced_data]
        unique_labels, counts = np.unique(labels, return_counts=True)
        print(f"æ ‡ç­¾åˆ†å¸ƒ: {dict(zip(unique_labels, counts))}")
        
        return balanced_data
    
    def tokenize_function(self, examples):
        """tokenizeå‡½æ•°"""
        tokenized = self.tokenizer(
            examples['text'],
            truncation=True,
            padding=True,
            max_length=self.max_length,
            return_tensors=None
        )
        
        # ç¡®ä¿labelså­—æ®µè¢«ä¿ç•™
        tokenized['labels'] = examples['label']
        
        return tokenized

def compute_debug_metrics(pred):
    """è®¡ç®—è°ƒè¯•è¯„ä¼°æŒ‡æ ‡"""
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    
    # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
    labels = labels.astype(int)
    preds = preds.astype(int)
    
    # è°ƒè¯•ä¿¡æ¯
    print(f"ğŸ” é¢„æµ‹è°ƒè¯•:")
    print(f"  æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(labels)}")
    print(f"  é¢„æµ‹åˆ†å¸ƒ: {np.bincount(preds)}")
    print(f"  å‰10ä¸ªæ ‡ç­¾: {labels[:10]}")
    print(f"  å‰10ä¸ªé¢„æµ‹: {preds[:10]}")
    
    # åŸºç¡€æŒ‡æ ‡
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary', zero_division=0)
    acc = accuracy_score(labels, preds)
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(labels, preds)
    print(f"æ··æ·†çŸ©é˜µ:\n{cm}")
    
    if cm.shape == (2, 2):
        tn, fp, fn, tp = cm.ravel()
    else:
        tp = np.sum((labels == 1) & (preds == 1))
        tn = np.sum((labels == 0) & (preds == 0))
        fp = np.sum((labels == 0) & (preds == 1))
        fn = np.sum((labels == 1) & (preds == 0))
    
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall,
        'true_negatives': int(tn),
        'false_positives': int(fp),
        'false_negatives': int(fn),
        'true_positives': int(tp),
        'eval_true_positives': int(tp),
        'eval_false_positives': int(fp),
        'eval_false_negatives': int(fn),
    }

def setup_debug_model_and_tokenizer(config: DebugTrainingConfig):
    """è®¾ç½®è°ƒè¯•æ¨¡å‹å’Œtokenizer"""
    print("åŠ è½½tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    
    print("åŠ è½½æ¨¡å‹...")
    model = AutoModelForSequenceClassification.from_pretrained(
        config.model_name,
        num_labels=2,
        ignore_mismatched_sizes=True
    )
    
    # é…ç½®LoRA
    print("é…ç½®LoRA...")
    lora_config = LoraConfig(
        r=config.lora_r,
        lora_alpha=config.lora_alpha,
        target_modules=["query", "value"],
        lora_dropout=config.lora_dropout,
        bias="none",
        task_type="SEQ_CLS"
    )
    
    model = get_peft_model(model, lora_config)
    
    # æ‰“å°å¯è®­ç»ƒå‚æ•°
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    all_params = sum(p.numel() for p in model.parameters())
    print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({trainable_params/all_params*100:.2f}%)")
    
    return model, tokenizer

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    
    # é…ç½®
    config = DebugTrainingConfig()
    
    # æ£€æŸ¥GPU
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆå§‹åŒ–wandb
    if config.use_wandb:
        try:
            wandb.init(project="comedy-chunk-detection", name="longformer-debug-training")
        except:
            print("Wandbåˆå§‹åŒ–å¤±è´¥ï¼Œç»§ç»­è®­ç»ƒ...")
    
    # åŠ è½½æ¨¡å‹å’Œtokenizer
    model, tokenizer = setup_debug_model_and_tokenizer(config)
    
    # å‡†å¤‡æ•°æ®é›†
    print("å‡†å¤‡æ•°æ®é›†...")
    dataset = DebugDataset(
        config.data_path, 
        tokenizer, 
        config.max_length, 
        config.context_window
    )
    
    # å‡†å¤‡å¥å­çº§æ•°æ®
    processed_data = dataset.prepare_sentence_level_data()
    
    # åˆ†å±‚åˆ†å‰²æ•°æ®
    if config.use_stratified_sampling:
        print("ä½¿ç”¨åˆ†å±‚é‡‡æ ·åˆ†å‰²æ•°æ®...")
        labels = [d['label'] for d in processed_data]
        
        # æ£€æŸ¥æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°é‡
        unique_labels, counts = np.unique(labels, return_counts=True)
        print(f"ç±»åˆ«åˆ†å¸ƒ: {dict(zip(unique_labels, counts))}")
        
        # å¦‚æœæŸä¸ªç±»åˆ«æ ·æœ¬å¤ªå°‘ï¼Œä½¿ç”¨éšæœºåˆ†å‰²
        if min(counts) < 2:
            print(f"è­¦å‘Šï¼šæŸä¸ªç±»åˆ«æ ·æœ¬æ•°é‡ä¸è¶³ï¼ˆæœ€å°‘{min(counts)}ä¸ªï¼‰ï¼Œåˆ‡æ¢åˆ°éšæœºåˆ†å‰²")
            train_data, val_data = train_test_split(
                processed_data, 
                test_size=1-config.train_split, 
                random_state=config.seed
            )
        else:
            train_data, val_data = train_test_split(
                processed_data, 
                test_size=1-config.train_split, 
                stratify=labels,
                random_state=config.seed
            )
    else:
        print("ä½¿ç”¨éšæœºåˆ†å‰²æ•°æ®...")
        train_data, val_data = train_test_split(
            processed_data, 
            test_size=1-config.train_split, 
            random_state=config.seed
        )
    
    print(f"è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬")
    print(f"éªŒè¯é›†: {len(val_data)} æ ·æœ¬")
    
    # æ˜¾ç¤ºåˆ†å‰²åçš„æ•°æ®åˆ†å¸ƒ
    train_pos = sum(d['label'] for d in train_data)
    train_neg = len(train_data) - train_pos
    val_pos = sum(d['label'] for d in val_data)
    val_neg = len(val_data) - val_pos
    
    print(f"è®­ç»ƒé›†åˆ†å¸ƒ: æ­£æ ·æœ¬ {train_pos} ({train_pos/len(train_data)*100:.1f}%), è´Ÿæ ·æœ¬ {train_neg} ({train_neg/len(train_data)*100:.1f}%)")
    print(f"éªŒè¯é›†åˆ†å¸ƒ: æ­£æ ·æœ¬ {val_pos} ({val_pos/len(val_data)*100:.1f}%), è´Ÿæ ·æœ¬ {val_neg} ({val_neg/len(val_data)*100:.1f}%)")
    
    # åˆ›å»ºHuggingFaceæ•°æ®é›†
    train_dataset = Dataset.from_list(train_data)
    val_dataset = Dataset.from_list(val_data)
    
    # Tokenizeæ•°æ®é›†
    train_dataset = train_dataset.map(
        dataset.tokenize_function, 
        batched=True, 
        remove_columns=train_dataset.column_names
    )
    val_dataset = val_dataset.map(
        dataset.tokenize_function, 
        batched=True, 
        remove_columns=val_dataset.column_names
    )
    
    # æ•°æ®æ”¶é›†å™¨
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
    
    # è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=config.output_dir,
        learning_rate=config.learning_rate,
        per_device_train_batch_size=config.batch_size,
        per_device_eval_batch_size=config.batch_size,
        num_train_epochs=config.num_epochs,
        weight_decay=config.weight_decay,
        eval_strategy="steps",
        eval_steps=10,  # æ¯10æ­¥è¯„ä¼°ä¸€æ¬¡
        save_strategy="steps",
        save_steps=10,  # æ¯10æ­¥ä¿å­˜ä¸€æ¬¡ï¼Œä¸è¯„ä¼°ç­–ç•¥åŒ¹é…
        load_best_model_at_end=True,
        metric_for_best_model="f1",
        greater_is_better=True,
        warmup_steps=config.warmup_steps,
        logging_steps=5,
        save_total_limit=2,
        report_to="wandb" if wandb.run is not None else None,
        remove_unused_columns=False,
        fp16=config.fp16,
        gradient_accumulation_steps=config.gradient_accumulation_steps,
        logging_dir="./logs",
        log_level="info",
        dataloader_pin_memory=False,
        dataloader_num_workers=0,
    )
    
    # åˆ›å»ºè°ƒè¯•å›è°ƒ
    debug_callback = DebugCallback()
    
    # åˆ›å»ºTrainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_debug_metrics,
        callbacks=[debug_callback],
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("å¼€å§‹è°ƒè¯•è®­ç»ƒ...")
    trainer.train()
    
    # ä¿å­˜æ¨¡å‹
    print("ä¿å­˜æ¨¡å‹...")
    trainer.save_model(config.model_save_dir)
    tokenizer.save_pretrained(config.model_save_dir)
    
    # æœ€ç»ˆè¯„ä¼°
    print("æœ€ç»ˆè¯„ä¼°...")
    results = trainer.evaluate()
    print(f"æœ€ç»ˆç»“æœ: {results}")
    
    # ä¿å­˜è®­ç»ƒç»“æœ
    with open("debug_training_results.json", "w") as f:
        json.dump(results, f, indent=2)
    
    print("è°ƒè¯•è®­ç»ƒå®Œæˆï¼")

if __name__ == "__main__":
    main() 